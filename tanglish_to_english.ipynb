{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dependencies"
      ],
      "metadata": {
        "id": "83RRcOfCysXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module importing"
      ],
      "metadata": {
        "id": "hqvFXGcSyFF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZXopeKwSkE1l"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading dataset"
      ],
      "metadata": {
        "id": "rHH9Xq-TyJRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read text files\n",
        "def read_sentences(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = [line.strip() for line in file]\n",
        "    return sentences\n",
        "tamil_sentences = read_sentences('train.txt')\n",
        "english_sentences = read_sentences('trainen.txt')"
      ],
      "metadata": {
        "id": "sVJisUzjmnXU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "aIJpaFIyyM7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Tamil sentences\n",
        "tamil_tokenizer = Tokenizer()\n",
        "tamil_tokenizer.fit_on_texts(tamil_sentences)\n",
        "tamil_sequences = tamil_tokenizer.texts_to_sequences(tamil_sentences)\n",
        "\n",
        "# Tokenize English sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_length = max(len(seq) for seq in tamil_sequences + english_sequences)\n",
        "tamil_padded = pad_sequences(tamil_sequences, maxlen=max_length, padding='post')\n",
        "english_padded = pad_sequences(english_sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "rcNYpsARxT_Q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "V5slHYRByQb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model architecture"
      ],
      "metadata": {
        "id": "MQOsiOLFy3WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tamil_tokenizer.word_index)+1, output_dim=256, input_length=max_length),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.RepeatVector(max_length),\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(english_tokenizer.word_index)+1, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qNGzeXsjkf8Q"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "zJKEAvG4yU_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tamil_padded, english_padded, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "id": "Qgju7s5ck65j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "dBDwBHL4yXE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss and accuracy"
      ],
      "metadata": {
        "id": "93qDBuW6y_Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(tamil_padded, english_padded)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "3qz7ctVglnTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation with example"
      ],
      "metadata": {
        "id": "jG1VhkGxybj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    sequence = tamil_tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    translation = model.predict(padded_sequence)\n",
        "    print(translation)\n",
        "    translation = tf.argmax(translation, axis=-1).numpy()[0]\n",
        "    print(translation)\n",
        "    translated_sentence = ' '.join([list(english_tokenizer.word_index.keys())[idx-1] for idx in translation if idx != 0])\n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "5i4vLIQjk9pW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tamil_sentence = \"vanakam\"\n",
        "translated_sentence = translate(tamil_sentence)\n",
        "print(f\"Tamil: {tamil_sentence}\")\n",
        "print(f\"English: {translated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELmjuTA7lC4s",
        "outputId": "9899587b-6d8f-443b-e21e-30d23a75103f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n",
            "[[[1.38579565e-03 9.94906306e-01 9.57807487e-08 4.19133812e-07\n",
            "   1.56862450e-06 6.87762986e-06 3.65471188e-03 4.43824392e-05]\n",
            "  [9.98389482e-01 6.29042741e-04 1.56911684e-09 1.07814566e-08\n",
            "   4.61561562e-08 3.89372588e-07 6.61383237e-05 9.14991717e-04]\n",
            "  [9.99983311e-01 3.00964234e-06 1.67736484e-11 6.76802364e-11\n",
            "   5.98433691e-10 1.05891971e-08 1.04758260e-06 1.26326495e-05]\n",
            "  [9.99993086e-01 9.61565547e-07 7.37199624e-12 2.55637421e-11\n",
            "   2.70523076e-10 4.41256010e-09 4.27448839e-07 5.57348176e-06]]]\n",
            "[1 0 0 0]\n",
            "Tamil: vanakam\n",
            "English: hello\n"
          ]
        }
      ]
    }
  ]
}